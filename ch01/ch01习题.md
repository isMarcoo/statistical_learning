# ch01习题

## 题一

说明伯努利模型的极大似然估计以及贝叶斯估计中的统计学习方法三要素。伯努利模型是定义在取值为0与1的随机变量上的概率分布。假设观测到伯努利模型$n$次独立的数据生成结果，其中$k$次的结果为1，这时可以用极大似然估计或贝叶斯估计来估计结果为1的概率。

解：

伯努利分布为：
$$
P(X=1)=p
\\
P(X=0)=1-p
$$
则用$X$表示事件1发生的次数，伯努利模型可以写为:
$$
P_p(X=x)=p^x(1-p)^{1-x}
$$
若p确定，则模型的参数确定，若p不确定，则其假设空间应该为：
$$
\mathcal{F}=\{P|P_p(X)=p^x(1-p)^{1-x},p\in[0,1]\}
$$
表示所有满足条件的函数$P$的集合，$P$为$p$的函数，$p$是参数。

**极大似然估计：**

极大似然估计其实就是要通过已知的数据（即$x$）去极大化似然，也就是说要去估计一个参数$\theta$来让这个模型生成这些数据的概率最大。其中$\theta$是变量。n重伯努利模型中其似然函数应该为：
$$
L(p|X)=\prod_{i=1}^nP(x^{(i)}|p)
$$
其中p为未知的参数，极大似然估计就是要求得一个p，使得似然函数最大。由于连乘不好计算，通常会使用对数似然函数求导来计算极大点（因为对数形式与原形式同单调性）。则继续运算为：
$$
\log L(p|X) = \log \prod_{i=1}^nP(x^{(i)}|p)\\
=\log p^k(1-p)^{n-k}\\
=\log p^k + \log (1-p)^{n-k}\\
=k\log p + {(n-k)}\log {(1-p)}\\
$$
其中k表示n次试验中1事件发生k次。对p求导得：
$$
\frac{\partial L}{\partial p}=\frac{k}{p}-\frac{n-k}{1-p}=0\\
k(1-p)-(n-k)p=0\\
p=\frac{k}{n}
$$
即求得的参数$p=\frac{k}{n}$.

**贝叶斯估计：**

> 极大似然估计和极大后验估计（贝叶斯派）以及贝叶斯估计的区别：
>
> **极大似然估计：**将参数$\theta $视为常规变量，即只是一个数。
>
> **极大后验估计：**将参数$\theta $视为随机变量（也即是频率派和贝叶斯派的最大区别），有自己的分布。但还是用优化的方法寻求最好参数。
>
> **贝叶斯估计：**在极大后验估计的基础上，不使用优化的方法，而是使用后验期望作为最优参数。

方法一（极大后验估计）：

>伯努利分布的先验分布为Beta分布，也即$\pi(p)$是Beta分布，这是由于Beta分布是伯努利分布的共轭分布，一般选用先验分布时会选用其共轭分布，这样先验和后验的分布形式相同，容易计算

伯努利分布的先验分布为：
$$
\pi(p)=\frac1{B(\alpha,\beta)}p^{(\alpha-1)}(1-p)^{\beta-1}
$$
似然函数为：
$$
\begin{aligned}
L(p|X)& =P(X|p) \\
&=\prod_{i=1}^nP(x^{(i)}|p) \\
&=p^k(1-p)^{n-k}
\end{aligned}
$$
根据贝叶斯公式，最大化后验概率，则：
$$
\begin{aligned}
\hat{p}& =\arg\max_p\frac{P(X|p)\pi(p)}{\int P(X|p)\pi(p)dp} \\
&=\arg\max_pP(X|p)\pi(p) \\
&=\arg\max_pp^k(1-p)^{n-k}\frac{1}{B(\alpha,\beta)}p^{(\alpha-1)}(1-p)^{\beta-1} \\
&=\arg\max_p\frac1{B(\alpha,\beta)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1}
\end{aligned}
$$
令$g(p)=\frac{1}{B(\alpha,\beta)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1}$，对函数$g(p)$先取对数，再对$p$求导，得
$$
\frac{\partial\log g(p)}{\partial p}=\frac1{B(\alpha,\beta)}\left(\frac{k+\alpha-1}p-\frac{n-k+\beta-1}{1-p}\right)
$$
求解得
$$
\hat{p}=\frac{k+\alpha-1}{n+\alpha+\beta-2}
$$


方法二（贝叶斯估计）：

在贝叶斯估计中应当使用后验期望做估计，所以需要计算后验分布的期望

后验分布为：
$$
\begin{aligned}
P(p|X)& =\frac{P(X|p)\pi(p)}{\int P(X|p)\pi(p)dp} \\
&=\frac{\frac1{B(\alpha,\beta)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1}}{\int\frac1{B(\alpha,\beta)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1}dp} \\
&=\frac{p^{k+\alpha-1}(1-p)^{n-k+\beta-1}}{\int p^{k+\alpha-1}(1-p)^{n-k+\beta-1}dp} \\
&=\frac1{B(k+\alpha,n-k+\beta)}p^{k+\alpha-1}(1-p)^{n-k+\beta-1} \\
&\sim\mathrm{Be}(k+\alpha,n-k+\beta)
\end{aligned}
$$
其期望为：
$$
\begin{aligned}E_p(p|X)&=E_p(\mathrm{Be}(k+\alpha,n-k+\beta))\\&=\frac{k+\alpha}{(k+\alpha)+(n-k+\beta)}\\&=\frac{k+\alpha}{n+\alpha+\beta}\end{aligned}
$$
则其估计值为
$$
\hat{p}=\frac{k+\alpha}{n+\alpha+\beta}
$$

> 以上计算过程参考于[datawhale统计学习方法习题解答开源项目](https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter01/ch01)

## 题二

通过经验风险最小化推导极大似然估计。证明模型是条件概率分布，当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。

解：

对数损失函数为
$$
L(Y,P(Y|X))=-\log P(Y|X)
$$
经验风险最小化过程为：
$$
\begin{aligned}\arg\min_{f\in\mathcal{F}}\frac1N\sum_{i=1}^NL(y_i,f(x_i))&=\arg\min_{f\in\mathcal{F}}\frac1N\sum_D[-\log P(Y|X)]\\&=\arg\max_{f\in\mathcal{F}}\frac1N\log\prod_D\log P(Y|X)\\&=\arg\max_{f\in\mathcal{F}}\frac1N\log\prod_DP(Y|X)\\&=\frac1N\arg\max_{f\in\mathcal{F}}\log\prod_DP(Y|X)\end{aligned}
$$
又根据似然函数的定义，发现两者形式相同，即经验风险最小化等价于极大似然估计。