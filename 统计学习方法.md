# 统计学习方法

## 生成模型VS判别模型

生成模型是建模联合分布

判别模型直接建模条件分布

## 核方法

把线性模型扩展到非线性模型的直接做法：从输入空间映射到特征空间，并在特征空间做点积

而核方法直接在输入空间中定义核函数$K(x_1, x_2)$使其满足$K(x_1,x_2)=<\phi(x_1),\phi(x_2)>$

## 经验风险VS期望风险

期望风险是在联合分布上计算的损失，我们一般希望模型学到的期望风险是最小的，但计算期望风险需要知道联合分布，而没办法直接知道联合分布。

经验风险是在训练集上计算的平均损失，根据大数定律，当N区域无穷大，经验风险趋于期望风险。

因此通常用经验风险估计期望风险。但由于估计通常不够好，所以用**经验风险最小化**和**结构风险最小化**两种策略。

## 经验风险最小化VS结构风险最小化

经验风险最小化顾名思义就是要在训练集上的平均损失最小（数据量大时效果不错）。

结构风险最小化其实是正则化技术，即添加了模型的复杂度损失，是为了避免过拟合现象（数据少量时），一般是参数向量的范数。

## 感知机

模型：
$$
f(x)=sign(wx+b)
$$
其中sign是符号函数：
$$
\operatorname{sign}(x)=\left\{\begin{array}{cc}+1,&x\geqslant0\\\\-1,&x<0\end{array}\right.
$$
是一种线性分类模型，也是判别模型。

## 感知机对偶形式的理解

感知机原始形式是直接作用与$w$向量，即遍历所有样本点，一旦碰到未分类正确的样本，则更新参数。直观上理解可以认为感知机模型每次更新参数只看分类错误的那一个样本点，而不会考虑其他的样本点。

感知机对偶形式想要将所有样本点都考虑在内，因此将$w$用所有$x$和$y$的线性组合来表示，其推理过程如下：

假设算法在训练过程中一共进行了$T$次更新，每次更新都是因为某个样本被分类错误，则第$T$次更新时，权重向量$w$可以表示为：
$$
\bold{w}^{(t)}=\bold{w}^{(t-1)}+\eta y_{i_t}\bold{x}_{i_t}
$$
我们可以将所有更新步骤的累积和表示为$\bold{w}$，则：
$$
\bold{w}=\sum_{t=1}^{T}\eta y_{i_t}\bold{x}_{i_t}
$$
我们将$\alpha_i$表示为$n_i\eta$。其中$n_i$表示第$i$个样本被误分类了几次，当$\eta$为1时，$\alpha_i$也将表示这个意义。从最终形式来看：
$$
\bold{w}=\sum_{t=1}^{T}\alpha_i y_{i_t}\bold{x}_{i_t}
$$
我们可以将$\alpha$认为是每一个样本的权重，这些样本线性组合得到最终的$\bold{w}$.

所以感知机的对偶形式其实就是训练过程中从更新$\bold{w}$变成了更新$\alpha$.

对偶形式需要更新的条件变为：
$$
y_i\left(\sum_{j=1}^N\alpha_jy_jx_j\bullet x_i+b\right)\leqslant0
$$
其中训练样本仅以内积的方式存在，所以可以在训练前先行计算，这个矩阵就是所谓的$Gram$矩阵。

## 朴素贝叶斯

朴素贝叶斯对条件概率分布做了条件独立性的假设，即当类别确定的时候，用于分类的特征是条件独立的，即训练数据的每一维特征之间是独立的。

朴素贝叶斯的分类公式：
$$
y=f(x)=\arg\max_{c_k}\frac{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_kP(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}
$$
其中，$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$是我们要学习的参数（要估计的参数），分别是先验和似然。

朴素贝叶斯中采用的后验概率最大化来分类其实是等价于期望风险最小化

证明：

如书中选择0-1损失函数：
$$
L(Y,f(X))=\begin{cases}1,&Y\neq f(X)\\\\0,&Y=f(X)\end{cases}
$$
则期望风险函数即对损失函数求期望，则为：
$$
R_{\exp}(f)=E[L(Y,f(X))]\\
R_{\exp}(f)=\sum_X\sum_YL(Y=c_k,X=x)P(Y=c_k,X=x)\\
R_{\exp}(f)=\sum_X\sum_YL(Y=c_k,X=x)P(Y=c_k|X=x)P(X)\\
R_{\exp}(f)=E_X\sum_YL(Y=c_k,X=x)P(Y=c_k|X)
$$
我们要最小化期望风险：
$$
\begin{aligned}f(x)&=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^{K}L(c_{k},y)P(c_{k}|X=x)\\&=\arg\min_{y\in\mathcal{Y}}\sum_{k=1}^{K}P(y\neq c_{k}|X=x)\\&=\arg\min_{y\in\mathcal{Y}}(1-P(y=c_{k}|X=x))\\&=\arg\max_{y\in\mathcal{Y}}P(y=c_{k}|X=x)\end{aligned}
$$
即最小化期望风险等价于最大化后验概率。

在进行参数估计时，直接使用频率来估计，这是因为假设了每个样本属于类别$c_k$的概率是独立的，而且只有属于和不属于两个事件，符合伯努利分布，因此可直接使用伯努利分布的极大似然估计，得到$p=\frac{N_{C_K}}{N}$。

**朴素贝叶斯法的贝叶斯估计：**

等价于拉普拉斯平滑，这是因为将要要估计的概率假设为多项式分布，则加入共轭先验就是加入狄利克雷分布。对于狄利克雷先验中的超参数$\alpha$只要取$\alpha=1$我们就得到了拉普拉斯平滑。
